{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27KpZJS42Jk0"
      },
      "outputs": [],
      "source": [
        "!pip install numpy pandas scikit-learn nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers requests"
      ],
      "metadata": {
        "id": "Sp4Gy0zLrKfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "!pip install --upgrade transformers\n"
      ],
      "metadata": {
        "id": "m8mrEPBN0OlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade --force-reinstall torch torchvision torchaudio transformers\n"
      ],
      "metadata": {
        "id": "JmpDFmkWD3sU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import joblib\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "# Path to save the model\n",
        "MODEL_PATH = \"fact_checking_model.pkl\"\n",
        "\n",
        "def load_model():\n",
        "    \"\"\"\n",
        "    Load the pre-trained model and tokenizer from Hugging Face.\n",
        "    \"\"\"\n",
        "    model_name = \"facebook/bart-large-mnli\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "    return tokenizer, model\n",
        "\n",
        "def save_model(model, tokenizer, model_path=MODEL_PATH):\n",
        "    \"\"\"\n",
        "    Save the model and tokenizer as a .pkl file using joblib.\n",
        "    \"\"\"\n",
        "    # Convert the model state_dict to a dictionary before saving\n",
        "    model_data = {\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"tokenizer\": tokenizer\n",
        "    }\n",
        "\n",
        "    joblib.dump(model_data, model_path)\n",
        "    print(f\"✅ Model saved successfully as {model_path}\")\n",
        "\n",
        "def load_saved_model(model_path=MODEL_PATH):\n",
        "    \"\"\"\n",
        "    Load the saved model and tokenizer from the .pkl file.\n",
        "    \"\"\"\n",
        "    model_name = \"facebook/bart-large-mnli\"\n",
        "\n",
        "    # Load the dictionary from the pkl file\n",
        "    model_data = joblib.load(model_path)\n",
        "\n",
        "    # Reload tokenizer and model\n",
        "    tokenizer = model_data[\"tokenizer\"]\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "    model.load_state_dict(model_data[\"model_state_dict\"])\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    print(\"✅ Model loaded successfully!\")\n",
        "    return tokenizer, model\n",
        "\n",
        "def predict_factuality(text, tokenizer, model):\n",
        "    \"\"\"\n",
        "    Predict the factuality of a given text.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    probabilities = softmax(outputs.logits, dim=-1)\n",
        "    labels = [\"Contradiction\", \"Neutral\", \"Entailment\"]\n",
        "    prediction = labels[torch.argmax(probabilities).item()]\n",
        "    confidence = torch.max(probabilities).item()\n",
        "\n",
        "    return prediction, confidence\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the model and save it as a .pkl file\n",
        "    tokenizer, model = load_model()\n",
        "    save_model(model, tokenizer)\n",
        "\n",
        "    # Load the saved model for verification\n",
        "    tokenizer, model = load_saved_model()\n",
        "\n",
        "    # Example prediction\n",
        "    test_text = \"Trump is the Prime Minister of India.\"\n",
        "    prediction, confidence = predict_factuality(test_text, tokenizer, model)\n",
        "    print(f\"Prediction: {prediction}, Confidence: {confidence:.2f}\")\n"
      ],
      "metadata": {
        "id": "0T4ua9IlPn5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f230ec1-5839-45f2-90b9-9e40c1990bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model saved successfully as fact_checking_model.pkl\n",
            "✅ Model loaded successfully!\n",
            "Prediction: Contradiction, Confidence: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import joblib\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "MODEL_PATH = \"fact_checking_model6.pkl\"\n",
        "\n",
        "def save_model():\n",
        "    \"\"\"\n",
        "    Load pre-trained model and tokenizer, then save them properly.\n",
        "    \"\"\"\n",
        "    model_name = \"facebook/bart-large-mnli\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "    # Save tokenizer and model separately\n",
        "    model_data = {\n",
        "        \"tokenizer_name\": model_name,  # Save only model name for tokenizer\n",
        "        \"model_state_dict\": model.state_dict()  # Save model weights\n",
        "    }\n",
        "\n",
        "    joblib.dump(model_data, MODEL_PATH)\n",
        "    print(f\"✅ Model saved successfully as {MODEL_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    save_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxFBQdQSGeDJ",
        "outputId": "21f5eea0-ddf2-4738-9617-76f433c2dca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model saved successfully as fact_checking_model6.pkl\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}